<!DOCTYPE HTML>
<html lang="en">
  <head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YD5L9JY3WZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YD5L9JY3WZ');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yudong Will Xu</title>
  
  <meta name="author" content="Yudong Will Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ§©</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yudong (Will) Xu</name>
              </p>
              <p>
                I am currently pursuing my PhD in <a href="https://www.mie.utoronto.ca/" target="_blank">Mechanical and Industrial Engineering</a> at 
                the <a href="https://www.utoronto.ca/" target="_blank">University of Toronto</a>, 
                under the guidance of <a href="https://d3m.mie.utoronto.ca/members/ssanner/" target="_blank">Professor Scott Sanner</a> and
                 <a href="https://scholar.google.com/citations?user=juqDWQMAAAAJ&hl=en" target="_blank">Professor Elias Khalil</a>. 
                 My academic journey began at the same institution, where I completed my Bachelor of Applied Science (BASc) in <a href="https://engsci.utoronto.ca/" target="_blank">Engineering Science majoring in Machine Intelligence</a> 
                 followed by a Master of Applied Science (MASc) from the Department of Mechanical and Industrial Engineering, 
                 also supervised by Professor Sanner and Professor Khalil. My Master's research focused on solving the <a href="https://github.com/fchollet/ARC-AGI" target="_blank">Abstraction and Reasoning Corpus (ARC-AGI)</a> using object-centric approaches.
              </p>

              <p style="text-align:center">
                <a href="mailto:wil.xu@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                <a href="resources/Will-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.ca/citations?user=aiBPHn0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xuwil/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/yudongxuwil">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/xuwil/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:75%;max-width:75%" src="resources/photo.png">
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on developing <b>planning algorithms for robot and robot teams to accomplish long-horizon tasks 
                in dynamic and uncertain environment</b>.
                I am particuarly interested in solving problems in <b>real-world robotics</b> and have worked on many different applications,
                 such as multi-robot assembly, autonomous surface vessels (ASV) for environmental monitoring, and self-driving cars. 
                I have also worked on continual learning for robot dynamics modeling and control.
              </p>
              <p>
                If you have any questions / want to collaborate, feel free to reach out and send me an email! 
                I am very excited to talk with more people and learn about your work!
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody>
        <hr style="margin-top: 10px; border: 1px solid black;">
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="hnetcrl_stop()" onmouseover="hnetcrl_start()">
            <td style="padding:20px;width:100%;vertical-align:right; text-align: center;">
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src='resources/paper-vitarc.png' width="600" style="display: block; margin: 0px;">
              </div>
              <div style="display: flex; justify-content: center; align-items: center;">
                <p style="margin-bottom: -20px; padding-bottom: 0px; text-align: center;">Enhancing the vision transformer (ViT) architecture to solve the Abstraction and Reasoning Corpus.</p>
              </div>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.06405" style="margin-top: -5px;">
                <papertitle>Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects (Preprint)</papertitle>
              </a>
              <br>
              <!-- <a href="https://x.com/WenhaoLi29">Wenhao Li</a>, -->
              Wenhao Li,
              <strong>Yudong Xu</strong>,
              Scott Sanner,
              Elias B. Khalil
              <!-- <a href="https://d3m.mie.utoronto.ca/members/ssanner/">Scott Sanner</a>, -->
              <!-- <a href="https://scholar.google.com/citations?user=juqDWQMAAAAJ&hl=en">Elias B. Khalil</a> -->
              <br>
              <a href="https://arxiv.org/abs/2410.06405">Paper</a>
              /
              <!-- <a href="https://github.com/khalil-research/1D-ARC">Data</a>
              / -->
              <a href="https://x.com/WenhaoLi29/status/1846217448678207556">Tweet</a>
              /
              <!-- <p>The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on visual reasoning in the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popular data-driven approach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT -- otherwise a state-of-the-art model for images -- fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose ViTARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific ViTARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, ViTARC provides a strong foundation for future research in visual reasoning using transformer-based architectures.</p> -->
<!--              <p>Enhancing the vision transformer (ViT) architecture to solve the Abstraction and Reasoning Corpus.</p>-->
            <hr style="margin-top: 5px; border: 0.5px solid black;">

            </td>
        </tr>


          <tr onmouseout="hnetcrl_stop()" onmouseover="hnetcrl_start()">
            <td style="padding:20px;width:100%;vertical-align:right; text-align: center;">
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src='resources/paper2.png' width="600" style="display: block; margin: auto;">
              </div>
              <div style="display: flex; justify-content: center; align-items: center;">
                <p style="margin-bottom: -20px; padding-bottom: 0px; text-align: center;">An exploration of LLMs' ability to solve the Abstraction and Reasoning Corpus.</p>
              </div>
            </td>
          </tr> 
          <tr> 
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=E8m8oySvPJ" style="margin-top: -5px;">
                <papertitle>LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations (TMLR-2024)</papertitle>
              </a>
              <br>
              <strong>Yudong Xu</strong>,
              Wenhao Li,
              Pashootan Vaezipoor,
              Scott Sanner,
              Elias B. Khalil
              <!-- <a href="https://x.com/WenhaoLi29">Wenhao Li</a>,
              <a href="https://www.cs.toronto.edu/~pashootan/">Pashootan Vaezipoor</a>,
              <a href="https://d3m.mie.utoronto.ca/members/ssanner/">Scott Sanner</a>,
              <a href="https://scholar.google.com/citations?user=juqDWQMAAAAJ&hl=en">Elias B. Khalil</a> -->
              <br>
              In Transactions on Machine Learning Research
              <br>
              <a href="https://khalil-research.github.io/LLM4ARC/">Website</a>
              /
              <a href="https://openreview.net/forum?id=E8m8oySvPJ">Paper</a>
              /
              <a href="https://github.com/khalil-research/1D-ARC">Data</a>
              /
              <a href="https://x.com/pashootan_vaezi/status/1666106834275430400">Tweet</a>
              /
              <p></p>
<!--              <p>An exploration of LLMs' ability to solve the Abstraction and Reasoning Corpus.</p>-->
              <!-- <p>Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to "reason" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability.</p> -->
            <hr style="margin-top: 5px; border: 0.5px solid black;">

            </td>
        </tr> 

        <tr onmouseout="hnetcrl_stop()" onmouseover="hnetcrl_start()">
          <td style="padding:20px;width:100%;vertical-align:middle;">
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src='resources/paper1.png' width="600" style="max-width: 100%;">
            </div>
            <div style="display: flex; justify-content: center; align-items: center;">
              <p style="margin-bottom: -20px; padding-bottom: 0px; text-align: center;">A graph-based symbolic AI approach to solving the Abstraction and Reasoning Corpus.</p>
            </div>
          </td>
        </tr> 
        <tr> 
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://doi.org/10.1609/aaai.v37i4.25527" style="margin-top: -5px;">
                <papertitle>Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus (AAAI-2023)</papertitle>
              </a>
              <br>
              <strong>Yudong Xu</strong>,
              Elias B. Khalil,
              Scott Sanner
              <!-- <a href="https://scholar.google.com/citations?user=juqDWQMAAAAJ&hl=en">Elias B. Khalil</a>, -->
              <!-- <a href="https://d3m.mie.utoronto.ca/members/ssanner/">Scott Sanner</a> -->
              <br>
              In Proceedings of the 37th AAAI Conference on Artificial Intelligence
              <br>
              <a href="https://doi.org/10.1609/aaai.v37i4.25527">Paper</a>
              /
              <a href="https://github.com/khalil-research/ARGA-AAAI23">Code</a>
              /
              <a href="resources/arga-poster.pdf">Poster</a>
              /
              <a href="resources/arga-presentation.pdf">Slides</a>
              /
              <p></p>
<!--              <p>A graph-based symbolic AI approach to solving the Abstraction and Reasoning Corpus.</p>-->
              <!-- <p>The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the performance of general artificial intelligence algorithms. The ARC's focus on broad generalization and few-shot learning has made it difficult to solve using pure machine learning. A more promising approach has been to perform program synthesis within an appropriately designed Domain Specific Language (DSL). However, these too have seen limited success. We propose Abstract Reasoning with Graph Abstractions (ARGA), a new object-centric framework that first represents images using graphs and then performs a search for a correct program in a DSL that is based on the abstracted graph space. The complexity of this combinatorial search is tamed through the use of constraint acquisition, state hashing, and Tabu search. An extensive set of experiments demonstrates the promise of ARGA in tackling some of the complicated object-centric tasks of the ARC rather efficiently, producing programs that are correct and easy to understand.</p> -->
<!--            <hr style="margin-top: 5px; border: 0.5px solid black;">-->

            </td>
        </tr>
<!--        <hr style="margin-top: 10px; border: 1px solid black;">-->
        </table>

<!--===============================================================        -->

<!-- Misc Section Header -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
    <tr>
      <td>
        <heading>Services</heading>
      </td>
    </tr>
<!--  </tbody>-->
  <hr style="margin-top: 10px; border: 1px solid black;">
<!--</table>-->

<!--&lt;!&ndash; Misc Section Content &ndash;&gt;-->
<!--<table width="100%" align="center" border="0" cellpadding="15">-->
<!--  <tbody>-->

    <!-- Reviewer Information -->
    <tr>
      <td style="margin-top: -10px;padding:40px;width:100%;vertical-align:middle;">
        Reviewer,
        <br>
        ICLR 2025, TMLR 2024,  IJCAI 2024
        <br>
        <br>

<!--        <br>-->
<!--        Teaching Assistant at University of Toronto-->
<!--        <br>-->
<!--        <a href="https://engineering.calendar.utoronto.ca/course/mie369h1">MIE369-Introduction to Artificial Intelligence</a>. Winter 2024, 2025-->

<!--        <strong>Reviewer for:</strong>-->
<!--        <ul style="margin-top: 5px; padding-left: 20px;">-->
<!--          <li>ICLR, 2025</li>-->
<!--          <li>TMLR, 2024</li>-->
<!--          <li>IJCAI, 2024</li>-->
<!--          <li>ACM TIST, 2024</li>-->
<!--          <li>JAIR, 2023</li>-->
<!--        </ul>-->
<!--      </td>-->
<!--    </tr>-->

<!--    &lt;!&ndash; Teaching Assistant Information &ndash;&gt;-->
<!--    <tr>-->
<!--      <td style="margin-top: -20px;padding:25px;width:100%;vertical-align:middle;">-->
        Teaching Assistant at the University of Toronto,
        <br>
<!--        <ul style="margin-top: 0px; padding-left: 0px;">-->
<!--          <li>-->
            <a href="https://engineering.calendar.utoronto.ca/course/mie369h1">
              MIE369 - Introduction to Artificial Intelligence
            </a>, Winter 2024, 2025
<!--          </li>-->
<!--        </ul>-->
      </td>
    </tr>

  </tbody>

</table>


<!-- Footer Section -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          This template is from Jon Barron's <a href="https://jonbarron.info/">website</a>.
          Here is the <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
        </p>
      </td>
    </tr>
  </tbody>
</table>







<!-- Misc Section Header -->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--  <tbody>-->
<!--    <tr>-->
<!--      <td>-->
<!--        <heading>Services</heading>-->
<!--      </td>-->
<!--    </tr>-->
<!--  </tbody>-->
<!--</table>-->

<!--&lt;!&ndash; Misc Section Content &ndash;&gt;-->
<!--<table width="100%" align="center" border="0" cellpadding="20">-->
<!--  <tbody>-->

<!--    &lt;!&ndash; Reviewer Information &ndash;&gt;-->
<!--    <tr>-->
<!--&lt;!&ndash;      <td style="padding:20px;width:27%;vertical-align:middle"></td>&ndash;&gt;-->
<!--      <td width="73%" valign="center">-->
<!--        Reviewer for-->
<!--        <br>-->
<!--&lt;!&ndash;        International Conference on Learning Representations, 2025&ndash;&gt;-->
<!--        ICLR, 2025-->
<!--        <br>-->
<!--&lt;!&ndash;        Transaction on Machine Learning Research, 2024&ndash;&gt;-->
<!--        TMLR, 2024-->
<!--        <br>-->
<!--&lt;!&ndash;        International Joint Conference on Artificial Intelligence, Survey Track, 2024&ndash;&gt;-->
<!--        IJCAI Survey Track, 2024-->
<!--        <br>-->
<!--&lt;!&ndash;        ACM Transactions on Intelligent Systems and Technology, 2024&ndash;&gt;-->
<!--        ACM TIST, 2024-->
<!--        <br>-->
<!--&lt;!&ndash;        The Journal of Artificial Intelligence Research, 2023&ndash;&gt;-->
<!--        JAIR, 2023-->
<!--        <br>-->
<!--      </td>-->
<!--    </tr>-->

<!--    &lt;!&ndash; Teaching Assistant Information &ndash;&gt;-->
<!--    <tr>-->
<!--&lt;!&ndash;      <td style="padding:20px;width:27%;vertical-align:middle"></td>&ndash;&gt;-->
<!--      <td width="73%" valign="center">-->
<!--        Teaching Assistant at University of Toronto-->
<!--        <br>-->
<!--        <a href="https://engineering.calendar.utoronto.ca/course/mie369h1">MIE369-Introduction to Artificial Intelligence</a>. Winter 2024, 2025-->
<!--      </td>-->
<!--    </tr>-->

<!--  </tbody>-->
<!--</table>-->


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:0px">-->
<!--              <br>-->
<!--              <p style="text-align:right;font-size:small;">-->
<!--                This template is from Jon Barron's <a href="https://jonbarron.info/">website</a>. Here is the <a href="https://github.com/jonbarron/jonbarron_website">source code</a>-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table> -->
<!--      </td>-->
<!--    </tr>-->
<!--  </table>-->




</body>

</html>
